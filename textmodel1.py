# -*- coding: utf-8 -*-
"""
CA vs LA â€” íì—… ì‹ë‹¹ ë¶€ì • ë¦¬ë·° n-gram ë¹„êµ (bi/tri-gram ë™ì‹œ)
- ë°ì´í„°: business_restaurants.parquet, review_restaurants.parquet
- ì ˆì°¨:
  1) íì—…(is_open=0) + ì£¼(state in ['CA','LA']) ë¦¬ë·°ë§Œ ì¶”ì¶œ
  2) VADERë¡œ ê°ì • ë¼ë²¨ë§ í›„ 'ë¶€ì •(neg)' ë¦¬ë·°ë§Œ ì‚¬ìš©
  3) í…ìŠ¤íŠ¸ ì •ì œ + ë¶ˆìš©ì–´/ê³µí†µ ë„ë©”ì¸ì–´ ì œê±°
  4) CountVectorizer(ngram_range=(2,3))ë¡œ bi/tri-gram ì¹´ìš´íŠ¸
  5) CA/LA ìƒëŒ€ ë¹ˆë„(log-odds with add-1 smoothing)ë¡œ ë¹„êµ
  6) ê° ì£¼ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ íŠ¹ì´í•œ n-gram Top 30 ì¶œë ¥ + CSV ì €ì¥
"""

from pathlib import Path
import re
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
import nltk
nltk.download('vader_lexicon')
nltk.download('stopwords')

from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.corpus import stopwords

# ==============================
# 0) ê²½ë¡œ/ë°ì´í„° ë¡œë“œ
# ==============================
BASE_DIR = Path(__file__).resolve().parent
business = pd.read_parquet(BASE_DIR / "business_restaurants.parquet")
review   = pd.read_parquet(BASE_DIR / "review_restaurants.parquet")

# ==============================
# 1) íì—… + ëŒ€ìƒ ì£¼ í•„í„° (CA, LA)
# ==============================
target_states = ["CA", "LA"]
closed_biz = business[(business["is_open"] == 0) & (business["state"].isin(target_states))][
    ["business_id", "state"]
]
df = review.merge(closed_biz, on="business_id", how="inner")[["business_id","state","text","date"]].copy()
df["text"] = df["text"].astype(str)

print(f"ğŸ“¦ íì—… ì‹ë‹¹ ë¦¬ë·° ìˆ˜ â€” CA: {(df['state']=='CA').sum():,} / LA: {(df['state']=='LA').sum():,}")

# ==============================
# 2) VADER ê°ì • ë¼ë²¨ë§ (ë¶€ì • ë¦¬ë·°ë§Œ ì‚¬ìš©)
# ==============================
# í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ(ìµœì´ˆ 1íšŒ)
try:
    _ = stopwords.words("english")
except LookupError:
    nltk.download("stopwords")
try:
    nltk.data.find("sentiment/vader_lexicon.zip")
except LookupError:
    nltk.download("vader_lexicon")

sia = SentimentIntensityAnalyzer()
POS_TH, NEG_TH = 0.05, -0.05

df["compound"] = df["text"].apply(lambda t: sia.polarity_scores(t)["compound"])
df["sentiment_label"] = np.where(df["compound"] > POS_TH, "pos",
                          np.where(df["compound"] < NEG_TH, "neg", "neu"))

neg = df[df["sentiment_label"]=="neg"].copy()
print(f"â˜ ï¸  ë¶€ì • ë¦¬ë·° ìˆ˜ â€” CA: {(neg['state']=='CA').sum():,} / LA: {(neg['state']=='LA').sum():,}")

# ==============================
# 3) ì „ì²˜ë¦¬ + ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´
# ==============================
stop_en = set(stopwords.words("english"))
# ë„ë©”ì¸ ê³µí†µì–´(êµ¬ë¶„ë ¥ ë‚®ì€ ì¼ë°˜ í† í°) ê°•ì œ ì œê±°
domain_stop = {
    "food","place","service","time","order","ordered","back","one","us","get","go","like","even","good","would",
    "got","really","just","also","still","bit","much","well","first","second","third","make","made","came","went",
    "im","ive","dont","didnt","wasnt","isnt","cant","couldnt","wont","theyre","youre","thats","its"
}
stop_all = stop_en.union(domain_stop)

def clean_for_ngram(t: str) -> str:
    t = t.lower()
    t = re.sub(r"[^a-z\s]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    tokens = [w for w in t.split() if (w not in stop_all and len(w) >= 2)]
    return " ".join(tokens)

neg["clean"] = neg["text"].apply(clean_for_ngram)

# ==============================
# 4) bi/tri-gram ë²¡í„°í™”
# ==============================
# min_df: ë„ˆë¬´ í¬ê·€í•œ n-gram ì œê±° (ë°ì´í„°ëŸ‰ì— ë§ì¶° ì¡°ì • ê°€ëŠ¥)
vec = CountVectorizer(ngram_range=(2,3), min_df=20, max_df=0.95)
X = vec.fit_transform(neg["clean"])
terms = np.array(vec.get_feature_names_out())

# ==============================
# 5) ì£¼ë³„ ì¹´ìš´íŠ¸ ë¶„í•´
# ==============================
mask_ca = (neg["state"]=="CA").values
X_ca = X[mask_ca]
X_la = X[~mask_ca]

ca_counts = np.asarray(X_ca.sum(axis=0)).ravel()
la_counts = np.asarray(X_la.sum(axis=0)).ravel()

# ==============================
# 6) log-odds with add-1 smoothing (ìƒëŒ€ì  ì„ í˜¸ë„)
# ==============================
ca_total = ca_counts.sum()
la_total = la_counts.sum()

# add-1 ìŠ¤ë¬´ë”©(ë¼í”Œë¼ìŠ¤) + ì „ì²´ ìš©ì–´ìˆ˜ ë³´ì •
V = len(terms)
ca_share = (ca_counts + 1) / (ca_total + V)
la_share = (la_counts + 1) / (la_total + V)

log_odds = np.log(ca_share / la_share)  # +ë©´ CA ì ë¦¼, -ë©´ LA ì ë¦¼

df_ng = pd.DataFrame({
    "ngram": terms,
    "ca_count": ca_counts,
    "la_count": la_counts,
    "ca_share": ca_share,
    "la_share": la_share,
    "log_odds": log_odds
})

# ë„ˆë¬´ í¬ê·€í•œ ê²ƒ ì œê±°(ê° ì£¼ ìµœì†Œ ë“±ì¥ íšŸìˆ˜ ê¸°ì¤€; í•„ìš” ì‹œ ì¡°ì •)
df_ng["min_count"] = df_ng[["ca_count","la_count"]].min(axis=1)
df_ng_f = df_ng[df_ng["min_count"] >= 30].copy()  # ì˜ˆ: ê° ì£¼ ìµœì†Œ 30íšŒ ì´ìƒ ë“±ì¥í•œ êµ¬ì ˆ

# ==============================
# 7) ê° ì£¼ íŠ¹ì´ êµ¬ì ˆ Top 30
# ==============================
top_CA = (df_ng_f.sort_values("log_odds", ascending=False)
                    .head(30))[["ngram","ca_count","la_count","log_odds"]]
top_LA = (df_ng_f.sort_values("log_odds", ascending=True)
                    .head(30))[["ngram","ca_count","la_count","log_odds"]]

print("\nğŸ”¥ CAì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ë” ë§ì´ ì“°ì¸ ë¶€ì • êµ¬ì ˆ (Top 30)")
print(top_CA.to_string(index=False))

print("\nğŸ”¥ LAì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ë” ë§ì´ ì“°ì¸ ë¶€ì • êµ¬ì ˆ (Top 30)")
print(top_LA.to_string(index=False))

# ==============================
# 8) CSV ì €ì¥(ì˜µì…˜)
# ==============================
out_dir = BASE_DIR / "outputs_ngram"
out_dir.mkdir(exist_ok=True)
top_CA.to_csv(out_dir / "neg_ngrams_CA_top30.csv", index=False)
top_LA.to_csv(out_dir / "neg_ngrams_LA_top30.csv", index=False)
df_ng_f.sort_values("log_odds", ascending=False).to_csv(out_dir / "neg_ngrams_all_logodds.csv", index=False)

print(f"\nğŸ“ ì €ì¥ ì™„ë£Œ: {out_dir/'neg_ngrams_CA_top30.csv'}")
print(f"ğŸ“ ì €ì¥ ì™„ë£Œ: {out_dir/'neg_ngrams_LA_top30.csv'}")
print(f"ğŸ“ ì €ì¥ ì™„ë£Œ: {out_dir/'neg_ngrams_all_logodds.csv'}")
