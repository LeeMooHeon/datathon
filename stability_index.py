# stability_index.py
from pathlib import Path
import pandas as pd
import numpy as np
import os

# =======================
# 0) ê³µí†µ ë¡œë” (ë¬´í—Œë‹˜ ë²„ì „)
# =======================
# ğŸ”¥ íŒŒì¼ì´ ìˆëŠ” í´ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ê³ ì •
BASE_DIR = Path(__file__).resolve().parent
os.chdir(BASE_DIR)  # í˜„ì¬ ì‘ì—… í´ë”ë¥¼ ì´ íŒŒì¼ì´ ìˆëŠ” ìœ„ì¹˜ë¡œ ë³€ê²½

def load_dataset(name):
    """
    Yelp Parquet ë¡œë”
    name: 'business', 'user', 'tip', 'check_in', 'review'
    """
    pq_path = f"{name}.parquet"

    if os.path.exists(pq_path):
        print(f"ğŸ“¦ {pq_path} ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        return pd.read_parquet(pq_path)

    json_path = f"{name}.json"
    if os.path.exists(json_path):
        print(f"ğŸ§© {json_path} â†’ Parquet ë³€í™˜ ì¤‘...")
        df = pd.read_json(json_path, lines=True)
        df.to_parquet(pq_path, compression="zstd", index=False)
        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {pq_path}")
        return df

    raise FileNotFoundError(f"{pq_path} ë˜ëŠ” {json_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# =======================
# 1) ì„¤ì •ê°’(ì´ˆê¸° íŒŒë¼ë¯¸í„° v1)
# =======================
CONFIG = {
    # Shock(ì €í‰ì  ì´ë²¤íŠ¸) íƒì§€ íŒŒë¼ë¯¸í„°
    "K_PRE": 20,        # ì´ë²¤íŠ¸ ì§ì „ ì°¸ì¡° ë¦¬ë·° ê°œìˆ˜(k)
    "M_EVENT": 3,       # ì´ë²¤íŠ¸ ì§í›„ í‰ê· ì— ì“¸ ë¦¬ë·° ê°œìˆ˜(m)
    "TAU_DROP": 0.8,    # ë“œë¡­ í­ ì„ê³„ê°’(Î” >= tauë©´ ì‡¼í¬ë¡œ ì¸ì •)
    # Recovery(íšŒë³µ) íŒì • íŒŒë¼ë¯¸í„°
    "ROLL_POST": 5,     # ì´ë²¤íŠ¸ í›„ ë¡¤ë§ í‰ê·  ê³„ì‚°ìš© ìœˆë„ìš°(ê°œìˆ˜ ê¸°ì¤€)
    "DELTA_RECOV": 0.2, # íšŒë³µ ëª©í‘œì„  í—ˆìš© ì˜¤ì°¨(|Î¼_post - Î¼_pre| <= Î´)
    "SUSTAIN_N": 5,     # ëª©í‘œì„  ë„ë‹¬ í›„ ìœ ì§€í•´ì•¼ í•˜ëŠ” ë¦¬ë·° ìˆ˜(S)
    "MAX_DAYS": 180,    # íšŒë³µ ì¸¡ì •ì„ ìœ„í•œ ìµœëŒ€ ì¶”ì  ì¼ìˆ˜(ê²€ì—´ í•œë„)
    # ì ìˆ˜ ê°€ì¤‘ì¹˜
    "W_TIME": 0.6,      # íšŒë³µì†ë„ ê°€ì¤‘ì¹˜
    "W_EXTENT": 0.4,    # íšŒë³µí­ ê°€ì¤‘ì¹˜
    "ALPHA": 0.5,       # ì•ˆì •ì§€ìˆ˜: (ê¸°ë³¸ì‹ ë¢°ë„) vs (íšŒë³µë ¥) ë¹„ì¤‘
    # ì €ì‹ ë¢° ì½”í˜¸íŠ¸ ìµœì†Œ ë¦¬ë·° ìˆ˜
    "MIN_REVIEWS": 25,
}

# =======================
# 2) ìœ í‹¸
# =======================
def robust_minmax(s: pd.Series, q_low=0.05, q_high=0.95) -> pd.Series:
    """ë°±ë¶„ìœ„ ê¸°ë°˜ (5~95%) ê°•ê±´ ì •ê·œí™”: 0~1ë¡œ ìŠ¤ì¼€ì¼"""
    lo, hi = s.quantile(q_low), s.quantile(q_high)
    s_clip = s.clip(lo, hi)
    den = (hi - lo) if (hi - lo) != 0 else 1.0
    return (s_clip - lo) / den

# =======================
# 3) ë¦¬ë·° ê¸°ë°˜ ê¸°ë³¸ ì§‘ê³„
# =======================
def aggregate_reviews(review: pd.DataFrame) -> pd.DataFrame:
    """
    business_id ë‹¨ìœ„ ê¸°ë³¸ í†µê³„:
    - stars_mean / stars_std / review_count
    - low_star_count (1~2ì ) / low_star_ratio
    """
    df = review[['business_id','stars','date']].copy()

    # ë‚ ì§œ íŒŒì‹±
    if not np.issubdtype(df['date'].dtype, np.datetime64):
        df['date'] = pd.to_datetime(df['date'], errors='coerce')

    df = df.dropna(subset=['business_id','stars','date'])
    df['is_low'] = (df['stars'] <= 2).astype(int)

    agg = df.groupby('business_id', as_index=False).agg(
        stars_mean=('stars','mean'),
        stars_std=('stars','std'),
        review_count=('stars','count'),
        low_star_count=('is_low','sum')
    )
    agg['stars_std'] = agg['stars_std'].fillna(0.0)
    agg['low_star_ratio'] = agg['low_star_count'] / agg['review_count'].replace(0, np.nan)
    agg['low_star_ratio'] = agg['low_star_ratio'].fillna(0.0)
    return agg

# =======================
# 4) Shock & RecoveryScore ê³„ì‚° (ì•ˆì „ê°€ë“œ í¬í•¨)
# =======================
def compute_recovery_score_per_business(df_biz: pd.DataFrame, cfg=CONFIG) -> float:
    """
    ë‹¨ì¼ businessì˜ ë¦¬ë·° ì‹œê³„ì—´ë¡œë¶€í„° RecoveryScore(0~1) ê³„ì‚°.
    (ì•ˆì „ê°€ë“œ í¬í•¨: ì¸ë±ìŠ¤ ê²½ê³„, ì „ë¶€ NaNì¸ ë¡¤ë§ ì²˜ë¦¬)
    """
    if df_biz is None or len(df_biz) == 0:
        return 0.0

    s = df_biz[['stars','date']].dropna().copy()
    if not np.issubdtype(s['date'].dtype, np.datetime64):
        s['date'] = pd.to_datetime(s['date'], errors='coerce')
    s = s.dropna(subset=['date']).sort_values('date').reset_index(drop=True)

    if len(s) < max(cfg["MIN_REVIEWS"], cfg["K_PRE"] + cfg["M_EVENT"] + cfg["SUSTAIN_N"]):
        return 0.0  # ë°ì´í„° ë¶€ì¡±

    stars = s['stars'].astype(float).to_numpy()
    dates = s['date'].to_numpy()

    K, M = cfg["K_PRE"], cfg["M_EVENT"]
    tau   = cfg["TAU_DROP"]
    roll  = cfg["ROLL_POST"]
    delta = cfg["DELTA_RECOV"]
    sustain = cfg["SUSTAIN_N"]
    max_days = np.timedelta64(cfg["MAX_DAYS"], 'D')

    # -----------------
    # 1) Shock íƒì§€
    # -----------------
    shock_idx = None
    mu_pre = None
    mu_event = None

    for i in range(K, len(stars) - M + 1):
        pre   = stars[i-K:i].mean()
        event = stars[i:i+M].mean()
        drop  = pre - event
        if drop >= tau:
            shock_idx = i
            mu_pre = pre
            mu_event = event
            break

    if shock_idx is None:
        return 0.0  # ì‡¼í¬ ì—†ìŒ

    # -----------------
    # 2) íšŒë³µ íƒìƒ‰
    # -----------------
    # ì´ë²¤íŠ¸ ì´í›„ êµ¬ê°„ì˜ ë¡¤ë§ í‰ê· 
    post_series = pd.Series(stars[shock_idx:])
    post_vals = post_series.rolling(roll, min_periods=roll).mean().to_numpy()
    post_idx_offset = shock_idx + (roll - 1)  # dates ìƒì˜ ì²« ìœ íš¨ ì¸ë±ìŠ¤

    t0_date = dates[shock_idx]
    t_rec_date = None

    # jëŠ” post_valsì—ì„œì˜ ì¸ë±ìŠ¤ (post_idx_offset + j) ê°€ dates ì¸ë±ìŠ¤ë¥¼ ë„˜ì§€ ì•Šê²Œ ì²´í¬
    for j in range(len(post_vals)):
        if np.isnan(post_vals[j]):
            continue

        idx_global = post_idx_offset + j
        if idx_global >= len(dates):
            break  # ê²½ê³„ ë³´í˜¸

        mu_post = post_vals[j]

        # íšŒë³µ ëª©í‘œì„  ë„ë‹¬?
        if abs(mu_post - mu_pre) <= delta:
            # ì§€ì†ì„± ì²´í¬: ì´í›„ sustain-1ê°œë„ ì¡°ê±´ ë§Œì¡±?
            ok = True
            cnt = 1
            k = j + 1
            while cnt < sustain:
                if k >= len(post_vals):
                    ok = False
                    break
                if np.isnan(post_vals[k]):
                    ok = False
                    break

                idx_global_k = post_idx_offset + k
                if idx_global_k >= len(dates):
                    ok = False
                    break

                if abs(post_vals[k] - mu_pre) > delta:
                    ok = False
                    break

                cnt += 1
                k += 1

            if ok:
                t_rec_date = dates[idx_global]
                break

        # ê²€ì—´(ì‹œê°„ í•œë„)
        idx_for_time = min(shock_idx + j, len(dates)-1)
        if (dates[idx_for_time] - t0_date) > max_days:
            break

    # -----------------
    # 3) ì†ë„ ì„±ë¶„ S_time
    # -----------------
    if t_rec_date is not None:
        days = (t_rec_date - t0_date) / np.timedelta64(1, 'D')
    else:
        days = cfg["MAX_DAYS"]  # ë¯¸íšŒë³µ â†’ ìµœëŒ€ì¼ìˆ˜ë¡œ ì·¨ê¸‰
    S_time = 1.0 / (1.0 + days / 60.0)  # 60ì¼ ìŠ¤ì¼€ì¼ë§

    # -----------------
    # 4) íƒ„ì„± ì„±ë¶„ S_elas (ì „ë¶€ NaN ë°©ì§€)
    # -----------------
    delta_drop = mu_pre - mu_event
    horizon_end = t0_date + max_days
    horizon_mask = (dates >= t0_date) & (dates <= horizon_end)

    post_h = stars[horizon_mask]
    if post_h.size >= roll:
        post_h_roll = pd.Series(post_h).rolling(roll, min_periods=roll).mean().to_numpy()
        valid = post_h_roll[~np.isnan(post_h_roll)]
        if valid.size > 0:
            mu_max = float(valid.max())
            S_elas = max(0.0, min(1.0, (mu_max - mu_event) / max(delta_drop, 1e-6)))
        else:
            S_elas = 0.0
    else:
        S_elas = 0.0

    # -----------------
    # 5) ê²°í•©
    # -----------------
    rec = CONFIG["W_TIME"] * S_time + CONFIG["W_EXTENT"] * S_elas
    return float(np.clip(rec, 0.0, 1.0))

def compute_recovery_scores(review: pd.DataFrame) -> pd.DataFrame:
    """business_idë³„ RecoveryScore(0~1) ê³„ì‚°"""
    df = review[['business_id','stars','date']].copy()
    if not np.issubdtype(df['date'].dtype, np.datetime64):
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
    df = df.dropna(subset=['business_id','stars','date'])

    scores = []
    for bid, grp in df.groupby('business_id', sort=False):
        rec = compute_recovery_score_per_business(grp, CONFIG)
        scores.append((bid, rec))
    return pd.DataFrame(scores, columns=['business_id','RecoveryScore'])

# =======================
# 5) StableIndex ì‚°ì¶œ
# =======================
def build_stable_index(business: pd.DataFrame, review: pd.DataFrame) -> pd.DataFrame:
    """
    StableIndex ìƒì„± íŒŒì´í”„ë¼ì¸
    1) ë¦¬ë·° ì§‘ê³„
    2) íšŒë³µ ì ìˆ˜
    3) ê¸°ë³¸ì‹ ë¢°ë„(Base) & íŒ¨ë„í‹° & ì •ê·œí™”
    4) ì•ˆì •ì§€ìˆ˜ ê²°í•©
    """
    agg = aggregate_reviews(review)
    rec = compute_recovery_scores(review)

    # ë³‘í•©
    out = agg.merge(rec, on='business_id', how='left')
    out['RecoveryScore'] = out['RecoveryScore'].fillna(0.0)

    # ê¸°ë³¸ì‹ ë¢°ë„ êµ¬ì„±
    out['Base'] = out['stars_mean'] * np.log1p(out['review_count'])
    # ë³€ë™ì„± íŒ¨ë„í‹°
    out['PenaltyVar'] = 1.0 / (1.0 + out['stars_std'])
    # ì €í‰ì  íŒ¨ë„í‹°
    out['PenaltyLow'] = 1.0 - out['low_star_ratio']

    # ì¢…í•© ë² ì´ìŠ¤
    out['BaseScore'] = out['Base'] * out['PenaltyVar'] * out['PenaltyLow']

    # ê°•ê±´ ì •ê·œí™”(0~1)
    out['BaseScore_norm'] = robust_minmax(out['BaseScore'])
    out['RecoveryScore_norm'] = out['RecoveryScore']  # ì´ë¯¸ 0~1

    # ì•ˆì • ì§€ìˆ˜
    a = CONFIG["ALPHA"]
    out['StableIndex'] = a * out['BaseScore_norm'] + (1 - a) * out['RecoveryScore_norm']

    # ì‹ ë¢°ë„ íƒœê·¸(ë¦¬ë·°ìˆ˜ ë¶€ì¡±)
    out['low_data_flag'] = (out['review_count'] < CONFIG["MIN_REVIEWS"]).astype(int)

    # business ê¸°ë³¸ ë©”íƒ€ì™€ í•©ì¹˜ê¸°(ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)
    cols_meta = ['business_id','name','state','is_open','categories']
    meta_cols = [c for c in cols_meta if c in business.columns]
    out = out.merge(business[meta_cols], on='business_id', how='left')

    # ì»¬ëŸ¼ ì •ë¦¬
    keep = [
        'business_id','name','state','is_open','categories',
        'review_count','stars_mean','stars_std','low_star_count','low_star_ratio',
        'Base','PenaltyVar','PenaltyLow','BaseScore','BaseScore_norm',
        'RecoveryScore','StableIndex','low_data_flag'
    ]
    keep = [c for c in keep if c in out.columns]
    return out[keep].sort_values('StableIndex', ascending=False).reset_index(drop=True)

# =======================
# 6) ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
# =======================
if __name__ == "__main__":
    # ì´ë¯¸ ê³„ì‚°ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê¸° (ì˜µì…˜)
    if os.path.exists("business_stability.parquet"):
        print("ğŸ“‚ ê¸°ì¡´ ì•ˆì • ì§€ìˆ˜ íŒŒì¼ ë°œê²¬ â†’ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
        result = pd.read_parquet("business_stability.parquet")
    else:
        business = load_dataset("business")
        review = load_dataset("review")
        print("âš™ï¸ ì•ˆì • ì§€ìˆ˜ ê³„ì‚° ì¤‘...")
        result = build_stable_index(business, review)

        # ì €ì¥ (ì „ì²´ Parquet + ì „ì²´ CSV + ìƒìœ„ 50 CSV)
        result.to_parquet("business_stability.parquet", compression="zstd", index=False)
        result.to_csv("business_stability.csv", index=False, encoding="utf-8-sig")  # âœ… ì „ì²´ CSV
        result.head(50).to_csv("business_stability_head50.csv", index=False, encoding="utf-8-sig")

    print("âœ… ì•ˆì • ì§€ìˆ˜ ì¤€ë¹„ ì™„ë£Œ â†’ business_stability.parquet / business_stability.csv / business_stability_head50.csv")
    print(result.head())
